{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mB_sFildiDh"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "303iHntmdiDj"
      },
      "source": [
        "# T81-558: Applications of Deep Neural Networks\n",
        "**Module 7: Generative Adversarial Networks**\n",
        "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
        "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5n2iv9udiDk"
      },
      "source": [
        "# Module 7 Material\n",
        "\n",
        "* Part 7.1: Introduction to GANs for Image and Data Generation [[Video]](https://www.youtube.com/watch?v=hZw-AjbdN5k&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_1_gan_intro.ipynb)\n",
        "* **Part 7.2: Train StyleGAN3 with your Own Images** [[Video]](https://www.youtube.com/watch?v=R546LYsQk5M&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_2_train_gan.ipynb)\n",
        "* Part 7.3: Exploring the StyleGAN Latent Vector [[Video]](https://www.youtube.com/watch?v=goQzp8QSb2s&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_3_latent_vector.ipynb)\n",
        "* Part 7.4: GANs to Enhance Old Photographs Deoldify [[Video]](https://www.youtube.com/watch?v=0OTd5GlHRx4&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_4_deoldify.ipynb)\n",
        "* Part 7.5: GANs for Tabular Synthetic Data Generation [[Video]](https://www.youtube.com/watch?v=yujdA46HKwA&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_07_5_tabular_synthetic.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ohl-hxAAaNp"
      },
      "source": [
        "# Google CoLab Instructions\n",
        "\n",
        "The following code ensures that Google CoLab is running the correct version of TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "Y8_-1h5ddiDp",
        "outputId": "c623348e-940c-48b5-be88-c5998b0e8314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yqlUD4sdiDk"
      },
      "source": [
        "# Part 7.2: Train StyleGAN3 with your Own Images\n",
        "\n",
        "Training GANs with StyleGAN is resource-intensive. For the high-resolution face GANs trained by NVIDIA, the researchers used computers with 8 high-end GPUs. The GPU used by NVIDIA is an A100, which has more memory and cores than the P100 or V100 offered by even Colab Pro+. In this part, we will use StyleGAN2 to train, rather than StyleGAN3. You can use networks trained with StyleGAN2 from StyleGAN3; however, they will not be trained as well as StyleGAN3.\n",
        "\n",
        "Unfortunately, StyleGAN3 is compute-intensive and will perform slowly on any GPU that is not the latest Ampere technology. Because Colab does not provide such technology, I am keeping the training guide at the StyleGAN2 level. Switching to StyleGAN3 is relatively easy, as will be pointed out later. \n",
        "\n",
        "Make sure that you are running this notebook with a GPU runtime. GANs can be trained with either Google Colab Free or Pro. I recommend at least the Pro version due to better GPU instances, longer runtimes, and timeouts. Additionally, the capability of Google Colab Pro to run in the background is valuable when training GANs, as you can close your browser or reboot your laptop while training continues.\n",
        "\n",
        "\n",
        "\n",
        "Your training data and trained neural networks will be stored to GDRIVE. For GANs, I lay out my GDRIVE like this:\n",
        "\n",
        "* ./data/gan/images - RAW images I wish to train on.\n",
        "* ./data/gan/datasets - Actual training datasets that I convert from the raw images.\n",
        "* ./data/gan/experiments - The output from StyleGAN2, my image previews, and saved network snapshots.\n",
        "\n",
        "The drive is mounted to the following location.\n",
        "\n",
        "```\n",
        "/content/drive/MyDrive/data\n",
        "```\n",
        "\n",
        "### What Sort of GPU do you Have?\n",
        "\n",
        "The type of GPU assigned to you by Colab will greatly affect your training time. Some sample times that I achieved with Colab are given here. I've found that Colab Pro generally starts you with a V100, however, if you run scripts non-stop for 24hrs straight for a few days in a row, you will generally be throttled back to a P100.\n",
        "\n",
        "* 1024x1024 - V100 - 566 sec/tick (CoLab Pro)\n",
        "* 1024x1024 - P100 - 1819 sec/tick (CoLab Pro)\n",
        "* 1024x1024 - T4 - 2188 sec/tick (CoLab Free)\n",
        "\n",
        "By comparison, a 1024x1024 GAN trained with StyleGAN3 on a V100 is 3087 sec/tick.\n",
        "\n",
        "If you use Google CoLab Pro, generally, it will not disconnect before 24 hours, even if you (but not your script) are inactive. Free CoLab WILL disconnect a perfectly good running script if you do not interact for a few hours. The following describes how to circumvent this issue.\n",
        "\n",
        "* [How to prevent Google Colab from disconnecting?](https://stackoverflow.com/questions/57113226/how-to-prevent-google-colab-from-disconnecting)\n",
        "\n",
        "### Set Up New Environment\n",
        "\n",
        "You will likely need to train for >24 hours. Colab will disconnect you. You must be prepared to restart training when this eventually happens. Training is divided into ticks, every so many ticks (50 by default) your neural network is evaluated and a snapshot is saved. When CoLab shuts down, all training after the last snapshot is lost. It might seem desirable to snapshot after each tick; however, this snapshotting process itself takes nearly an hour. It is important to learn an optimal snapshot-size for your resolution and training data.\n",
        "\n",
        "We will mount GDRIVE so that your snapshots are saved there. You must also place your training images in GDRIVE.\n",
        "\n",
        "You must also install NVIDIA StyleGAN2 ADA PyTorch. We also need to downgrade PyTorch to a version that supports StyleGAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LVr5sV8CKz0",
        "outputId": "148d2f6e-794a-4224-fb70-98d40f5760a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.1 (from versions: 1.9.0, 1.10.0, 1.10.1)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==1.8.1\u001b[0m\n",
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 2.33 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n",
            "Requirement already satisfied: ninja in /Users/jeff/miniforge3/lib/python3.9/site-packages (1.10.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.8.1 torchvision==0.9.1\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "Jr6AwWuiuf9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBwmbfKJCORs"
      },
      "source": [
        "### Find Your Files\n",
        "\n",
        "The drive is mounted to the following location.\n",
        "\n",
        "```\n",
        "/content/drive/MyDrive/data\n",
        "```\n",
        "\n",
        "It might be helpful to use an ```ls``` command to establish the exact path for your images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwpXtlrCCRHe"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/data/gan/images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rl6ShUWrCUrV"
      },
      "source": [
        "### Convert Your Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOK8m0N-Caqa"
      },
      "outputs": [],
      "source": [
        "!python /content/stylegan2-ada-pytorch/dataset_tool.py --source /content/drive/MyDrive/data/gan/images --dest /content/drive/MyDrive/data/gan/dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDbsiP35CdgK"
      },
      "source": [
        "The following command can be used to clear out the newly created dataset.  If something goes wrong and you need to clean up your images and rerun the above command, you should delete your partially created dataset directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpF072wiCgys"
      },
      "outputs": [],
      "source": [
        "#!rm -R /content/drive/MyDrive/data/gan/dataset/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7sUkkb-CkrS"
      },
      "source": [
        "# Clean Up your Images\n",
        "\n",
        "It is important that all images have the same dimensions and color depth.  This code can identify images that have issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuWfEfEoColz"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "IMAGE_PATH = '/content/drive/MyDrive/data/gan/images/'\n",
        "files = [f for f in listdir(IMAGE_PATH) if isfile(join(IMAGE_PATH, f))]\n",
        "\n",
        "base_size = None\n",
        "for file in tqdm(files):\n",
        "  file2 = os.path.join(IMAGE_PATH,file)\n",
        "  img = Image.open(file2)\n",
        "  sz = img.size\n",
        "  if base_size and sz!=base_size:\n",
        "    print(f\"Inconsistant size: {file2}\")\n",
        "  elif img.mode=='RGB':\n",
        "    print(f\"Inconsistant color format: {file2}\")\n",
        "  else:\n",
        "    base_size = sz\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HyFZgnjCsPb"
      },
      "source": [
        "### Perform Initial Training\n",
        "\n",
        "This code performs the initial training.  Set SNAP low enough that you get a snapshot before Colab forces you to quit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-ZBcql7Cv14",
        "outputId": "95644f00-ed98-4e5d-b183-8e26da6693ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"metrics\": [\n",
            "    \"fid50k_full\"\n",
            "  ],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/data/gan/dataset\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 5250,\n",
            "    \"xflip\": false,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 16384,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.0025,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.8192\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 16,\n",
            "  \"batch_gpu\": 16,\n",
            "  \"ema_kimg\": 5.0,\n",
            "  \"ema_rampup\": 0.05,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"run_dir\": \"/content/drive/MyDrive/data/gan/experiments/00001-dataset-auto1\"\n",
            "}\n",
            "\n",
            "Output directory:   /content/drive/MyDrive/data/gan/experiments/00001-dataset-auto1\n",
            "Training data:      /content/drive/MyDrive/data/gan/dataset\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   5250\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    False\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "Num images:  5250\n",
            "Image shape: [1, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape         Datatype\n",
            "---                   ---         ---      ---                  ---     \n",
            "mapping.fc0           262656      -        [16, 512]            float32 \n",
            "mapping.fc1           262656      -        [16, 512]            float32 \n",
            "mapping               -           512      [16, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    263169      -        [16, 1, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [16, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [16, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    263169      -        [16, 1, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [16, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [16, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [16, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   263169      -        [16, 1, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [16, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [16, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [16, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   263169      -        [16, 1, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [16, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [16, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   1442561     4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.conv1   721409      4112     [16, 256, 64, 64]    float16 \n",
            "synthesis.b64.torgb   131585      -        [16, 1, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [16, 256, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [16, 256, 64, 64]    float32 \n",
            "synthesis.b128.conv0  426369      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.conv1  213249      16400    [16, 128, 128, 128]  float16 \n",
            "synthesis.b128.torgb  65793       -        [16, 1, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [16, 128, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [16, 128, 128, 128]  float32 \n",
            "synthesis.b256.conv0  139457      65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.conv1  69761       65552    [16, 64, 256, 256]   float16 \n",
            "synthesis.b256.torgb  32897       -        [16, 1, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [16, 64, 256, 256]   float16 \n",
            "synthesis.b256:1      -           -        [16, 64, 256, 256]   float32 \n",
            "---                   ---         ---      ---                  ---     \n",
            "Total                 23186516    175568   -                    -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   128         16       [16, 64, 256, 256]   float16 \n",
            "b256.skip      8192        16       [16, 128, 128, 128]  float16 \n",
            "b256.conv0     36928       16       [16, 64, 256, 256]   float16 \n",
            "b256.conv1     73856       16       [16, 128, 128, 128]  float16 \n",
            "b256           -           16       [16, 128, 128, 128]  float16 \n",
            "b128.skip      32768       16       [16, 256, 64, 64]    float16 \n",
            "b128.conv0     147584      16       [16, 128, 128, 128]  float16 \n",
            "b128.conv1     295168      16       [16, 256, 64, 64]    float16 \n",
            "b128           -           16       [16, 256, 64, 64]    float16 \n",
            "b64.skip       131072      16       [16, 512, 32, 32]    float16 \n",
            "b64.conv0      590080      16       [16, 256, 64, 64]    float16 \n",
            "b64.conv1      1180160     16       [16, 512, 32, 32]    float16 \n",
            "b64            -           16       [16, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [16, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [16, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [16, 512, 16, 16]    float16 \n",
            "b32            -           16       [16, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [16, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \n",
            "b16            -           16       [16, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [16, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \n",
            "b8             -           16       [16, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [16, 512]            float32 \n",
            "b4.out         513         -        [16, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          24000961    416      -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 1m 11s       sec/tick 10.4    sec/kimg 651.96  maintenance 60.5   cpumem 4.07   gpumem 11.15  augment 0.000\n",
            "Evaluating metrics...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "{\"results\": {\"fid50k_full\": 325.0048693553262}, \"metric\": \"fid50k_full\", \"total_time\": 654.1190233230591, \"total_time_str\": \"10m 54s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000000.pkl\", \"timestamp\": 1646353517.7779443}\n",
            "tick 1     kimg 4.0      time 16m 45s      sec/tick 273.9   sec/kimg 68.48   maintenance 659.9  cpumem 4.63   gpumem 9.38   augment 0.002\n",
            "tick 2     kimg 8.0      time 21m 20s      sec/tick 275.0   sec/kimg 68.75   maintenance 0.1    cpumem 4.63   gpumem 4.77   augment 0.004\n",
            "tick 3     kimg 12.0     time 25m 54s      sec/tick 274.5   sec/kimg 68.63   maintenance 0.1    cpumem 4.63   gpumem 4.78   augment 0.010\n",
            "tick 4     kimg 16.0     time 30m 31s      sec/tick 276.2   sec/kimg 69.06   maintenance 0.1    cpumem 4.63   gpumem 4.78   augment 0.014\n",
            "tick 5     kimg 20.0     time 35m 07s      sec/tick 275.8   sec/kimg 68.95   maintenance 0.1    cpumem 4.63   gpumem 4.78   augment 0.018\n",
            "tick 6     kimg 24.0     time 39m 43s      sec/tick 275.8   sec/kimg 68.96   maintenance 0.1    cpumem 4.63   gpumem 4.78   augment 0.021\n",
            "tick 7     kimg 28.0     time 44m 19s      sec/tick 275.7   sec/kimg 68.92   maintenance 0.1    cpumem 4.63   gpumem 4.78   augment 0.024\n",
            "tick 8     kimg 32.0     time 48m 55s      sec/tick 276.0   sec/kimg 69.01   maintenance 0.1    cpumem 4.63   gpumem 4.80   augment 0.029\n",
            "tick 9     kimg 36.0     time 53m 30s      sec/tick 275.0   sec/kimg 68.74   maintenance 0.1    cpumem 4.63   gpumem 4.78   augment 0.032\n",
            "tick 10    kimg 40.0     time 58m 06s      sec/tick 275.7   sec/kimg 68.93   maintenance 0.1    cpumem 4.63   gpumem 4.79   augment 0.037\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 294.7980491779325}, \"metric\": \"fid50k_full\", \"total_time\": 623.6222772598267, \"total_time_str\": \"10m 24s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000040.pkl\", \"timestamp\": 1646356905.2458718}\n",
            "tick 11    kimg 44.0     time 1h 13m 14s   sec/tick 275.4   sec/kimg 68.84   maintenance 632.5  cpumem 4.86   gpumem 4.78   augment 0.042\n",
            "tick 12    kimg 48.0     time 1h 17m 49s   sec/tick 275.4   sec/kimg 68.84   maintenance 0.1    cpumem 4.86   gpumem 4.81   augment 0.049\n",
            "tick 13    kimg 52.0     time 1h 22m 25s   sec/tick 275.3   sec/kimg 68.82   maintenance 0.1    cpumem 4.86   gpumem 4.78   augment 0.055\n",
            "tick 14    kimg 56.0     time 1h 27m 00s   sec/tick 275.8   sec/kimg 68.95   maintenance 0.1    cpumem 4.86   gpumem 4.79   augment 0.058\n",
            "tick 15    kimg 60.0     time 1h 31m 36s   sec/tick 275.5   sec/kimg 68.88   maintenance 0.1    cpumem 4.86   gpumem 4.80   augment 0.063\n",
            "tick 16    kimg 64.0     time 1h 36m 12s   sec/tick 275.5   sec/kimg 68.88   maintenance 0.1    cpumem 4.86   gpumem 4.80   augment 0.069\n",
            "tick 17    kimg 68.0     time 1h 40m 47s   sec/tick 274.5   sec/kimg 68.64   maintenance 0.1    cpumem 4.86   gpumem 4.80   augment 0.074\n",
            "tick 18    kimg 72.0     time 1h 45m 22s   sec/tick 275.3   sec/kimg 68.82   maintenance 0.1    cpumem 4.86   gpumem 4.79   augment 0.081\n",
            "tick 19    kimg 76.0     time 1h 49m 57s   sec/tick 274.8   sec/kimg 68.69   maintenance 0.1    cpumem 4.86   gpumem 4.79   augment 0.085\n",
            "tick 20    kimg 80.0     time 1h 54m 32s   sec/tick 275.5   sec/kimg 68.87   maintenance 0.1    cpumem 4.86   gpumem 4.79   augment 0.090\n",
            "Evaluating metrics...\n",
            "{\"results\": {\"fid50k_full\": 305.1609217572026}, \"metric\": \"fid50k_full\", \"total_time\": 622.2901954650879, \"total_time_str\": \"10m 22s\", \"num_gpus\": 1, \"snapshot_pkl\": \"network-snapshot-000080.pkl\", \"timestamp\": 1646360290.9641223}\n",
            "tick 21    kimg 84.0     time 2h 09m 40s   sec/tick 275.6   sec/kimg 68.91   maintenance 631.5  cpumem 4.84   gpumem 4.80   augment 0.095\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Modify these to suit your needs\n",
        "EXPERIMENTS = \"/content/drive/MyDrive/data/gan/experiments\"\n",
        "DATA = \"/content/drive/MyDrive/data/gan/dataset\"\n",
        "SNAP = 10\n",
        "\n",
        "# Build the command and run it\n",
        "cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --outdir {EXPERIMENTS} --data {DATA}\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhOQRZrJDCvA"
      },
      "source": [
        "# Resume Training\n",
        "\n",
        "You can now resume training after you are interrupted by something in the pervious step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1TFgr9MDJlS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Modify these to suit your needs\n",
        "EXPERIMENTS = \"/content/drive/MyDrive/data/gan/experiments\"\n",
        "NETWORK = \"network-snapshot-000080.pkl\"\n",
        "RESUME = os.path.join(EXPERIMENTS, \"00001-dataset-auto1\", NETWORK)\n",
        "DATA = \"/content/drive/MyDrive/data/gan/dataset\"\n",
        "SNAP = 10\n",
        "\n",
        "# Build the command and run it\n",
        "cmd = f\"/usr/bin/python3 /content/stylegan2-ada-pytorch/train.py --snap {SNAP} --resume {RESUME} --outdir {EXPERIMENTS} --data {DATA}\"\n",
        "!{cmd}"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of t81_558_class_07_2_train_gan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9 (tensorflow)",
      "language": "python",
      "name": "tensorflow"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}